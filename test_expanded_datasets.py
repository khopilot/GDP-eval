#!/usr/bin/env python3
"""
Test and Demonstrate Expanded GDP-eval Datasets
Author: Nicolas Delrieu
Contact: +855 92 332 554
"""

import sys
import json
from pathlib import Path
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

from src.data.dataset_loader import DatasetLoader, DatasetType, DatasetFilter

def print_section(title):
    print(f"\n{'='*80}")
    print(f"  {title}")
    print(f"{'='*80}")

def demonstrate_dataset_loading():
    """Demonstrate loading and analyzing expanded datasets"""

    print("""
ğŸ¯ GDP-eval EXPANDED DATASET DEMONSTRATION
================================================================================
Author: Nicolas Delrieu, AI Consultant (+855 92 332 554)
Showcasing 639 comprehensive test items across 8 major datasets
================================================================================
    """)

    # Initialize loader
    loader = DatasetLoader(base_path="data")

    # 1. Load all datasets and show statistics
    print_section("ğŸ“Š LOADING ALL DATASETS")
    all_data = loader.load_all_datasets()

    print("\nâœ… Successfully loaded datasets:")
    for name, data in all_data.items():
        if "tasks" in data:
            count = len(data["tasks"])
            item_type = "tasks"
        elif "tests" in data:
            count = len(data["tests"])
            item_type = "tests"
        elif "scenarios" in data:
            count = len(data["scenarios"])
            item_type = "scenarios"
        else:
            count = 0
            item_type = "items"
        print(f"   â€¢ {name:<30} {count:>4} {item_type}")

    # 2. Get comprehensive statistics
    print_section("ğŸ“ˆ DATASET STATISTICS")
    stats = loader.get_statistics()

    print(f"\nğŸ”¢ Overall Statistics:")
    print(f"   â€¢ Total items loaded: {stats['total_items']}")
    print(f"   â€¢ Datasets in memory: {stats['datasets_loaded']}")
    print(f"   â€¢ Cache entries: {stats['cache_size']}")

    if stats['language_distribution']:
        print(f"\nğŸŒ Language Distribution:")
        for lang, count in stats['language_distribution'].items():
            print(f"   â€¢ {lang}: {count:.0f} items")

    if stats['complexity_distribution']:
        print(f"\nğŸ“Š Complexity Distribution:")
        for level, count in sorted(stats['complexity_distribution'].items()):
            print(f"   â€¢ Level {level}: {count} items")

    if stats['category_distribution']:
        print(f"\nğŸ·ï¸ Category Distribution:")
        top_categories = sorted(stats['category_distribution'].items(),
                               key=lambda x: x[1], reverse=True)[:10]
        for category, count in top_categories:
            print(f"   â€¢ {category}: {count} items")

    # 3. Demonstrate filtering capabilities
    print_section("ğŸ” FILTERING DEMONSTRATION")

    # Filter professional tasks by sector
    print("\nğŸ“Œ Finance Sector Tasks (complexity 3-5):")
    finance_tasks = loader.load_professional_tasks(
        sectors=["finance"],
        complexity=(3, 5),
        limit=5
    )
    for i, task in enumerate(finance_tasks, 1):
        print(f"   {i}. {task.get('title', 'N/A')} (Level {task.get('complexity', 0)})")

    # Filter evaluation tests by language
    print("\nğŸ“Œ Khmer Language Tests:")
    khmer_tests = loader.load_evaluation_tests(
        languages=["khmer"],
        limit=5
    )
    for test_type, tests in khmer_tests.items():
        if tests:
            print(f"   â€¢ {test_type}: {len(tests)} tests")

    # 4. Validate all datasets
    print_section("âœ… DATASET VALIDATION")
    validation = loader.validate_datasets()

    all_valid = True
    for dataset, is_valid in validation.items():
        status = "âœ“" if is_valid else "âœ—"
        print(f"   {status} {dataset}")
        if not is_valid:
            all_valid = False

    if all_valid:
        print("\nğŸ‰ All datasets validated successfully!")

    # 5. Show metadata for each dataset
    print_section("ğŸ“‹ DATASET METADATA")

    all_metadata = loader.get_metadata()
    for name, metadata in all_metadata.items():
        if metadata:
            print(f"\nğŸ“ {name}:")
            print(f"   â€¢ Version: {metadata.version}")
            print(f"   â€¢ Test Count: {metadata.test_count}")
            print(f"   â€¢ File Hash: {metadata.file_hash[:8]}...")
            print(f"   â€¢ Loaded At: {metadata.loaded_at}")

    # 6. Demonstrate specific dataset features
    print_section("ğŸ¯ SPECIAL FEATURES SHOWCASE")

    # Load Khmer evaluation suite
    try:
        khmer_data = loader.load_dataset(DatasetType.KHMER_TESTS)
        print("\nğŸ‡°ğŸ‡­ Khmer Language Evaluation Suite:")
        if "metadata" in khmer_data:
            meta = khmer_data["metadata"]
            if "test_categories" in meta:
                for category, count in meta["test_categories"].items():
                    print(f"   â€¢ {category}: {count} tests")
    except:
        print("   âš ï¸ Khmer evaluation suite not found")

    # Load behavioral tests (HHH framework)
    try:
        behavioral_filter = DatasetFilter(categories=["helpfulness"])
        behavioral_data = loader.load_dataset(
            DatasetType.BEHAVIORAL_TESTS,
            behavioral_filter
        )
        helpful_tests = behavioral_data.get("tests", [])
        print(f"\nğŸ¤ Helpfulness Tests (HHH Framework): {len(helpful_tests)} tests")
    except:
        print("   âš ï¸ Behavioral tests not found")

    # 7. Performance metrics
    print_section("âš¡ PERFORMANCE METRICS")

    import time

    # Measure loading time
    start = time.time()
    loader.clear_cache()
    loader.load_dataset(DatasetType.CAPABILITY_TESTS)
    cold_time = time.time() - start

    start = time.time()
    loader.load_dataset(DatasetType.CAPABILITY_TESTS)  # Should hit cache
    warm_time = time.time() - start

    print(f"\nâ±ï¸ Loading Performance:")
    print(f"   â€¢ Cold load: {cold_time:.3f}s")
    print(f"   â€¢ Cached load: {warm_time:.3f}s")
    print(f"   â€¢ Speedup: {cold_time/warm_time:.1f}x")

    # Summary
    print(f"""
================================================================================
ğŸ† DATASET EXPANSION SUMMARY
================================================================================
âœ… Successfully created and loaded 639 comprehensive test items
âœ… 8 major datasets with full metadata and versioning
âœ… Multi-language support (English, Khmer, Mixed)
âœ… Advanced filtering and caching capabilities
âœ… Enterprise-grade data loading infrastructure
âœ… Ready for production AI evaluation

ğŸ“ Contact: Nicolas Delrieu â€¢ +855 92 332 554
ğŸ¯ GDP-eval: Measuring AI Impact on Cambodia's Digital Economy
================================================================================
    """)

if __name__ == "__main__":
    try:
        demonstrate_dataset_loading()
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()